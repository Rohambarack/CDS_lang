{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.3-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ucloud/.local/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ucloud/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.3-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, wrapt, tzdata, threadpoolctl, pillow, numpy, kiwisolver, joblib, fonttools, cycler, smart-open, scipy, pandas, contourpy, scikit-learn, matplotlib, gensim\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.50.0 gensim-4.3.2 joblib-1.3.2 kiwisolver-1.4.5 matplotlib-3.8.3 numpy-1.26.4 pandas-2.2.1 pillow-10.2.0 pytz-2024.1 scikit-learn-1.4.1.post1 scipy-1.12.0 smart-open-7.0.3 threadpoolctl-3.4.0 tzdata-2024.1 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "os.system(\"pip install gensim pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "import gensim.downloader as api\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"english_word2vec.bin\"\n",
    "model_path = os.path.join(\"..\",\"in\",\"model\",model_name)\n",
    "model = gensim.models.KeyedVectors.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "data_name = \"data_spoty.csv\"\n",
    "data_path = os.path.join(\"..\",\"in\",\"data\",data_name)\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset\n",
    "singer = \"ABBA\"\n",
    "sub_data = data.loc[data['artist'] == singer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed\n",
    "query_word = \"love\"\n",
    "n_similar = 12\n",
    "query_array = model.most_similar(query_word,topn=n_similar)\n",
    "query_array.append((query_word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find in subset (subset rows by indexing row number to sub_data.loc)\n",
    "max_row = len(sub_data.index)\n",
    "occurence_count_full = {\"song\":[]}\n",
    "for row_index in range(0,max_row):\n",
    "    row = sub_data.loc[row_index]\n",
    "    song_title = row[\"song\"]\n",
    "    song_lyric = row[\"text\"]\n",
    "    artist = row[\"artist\"]\n",
    "    #clean lyrics to be compatible with the word embedding labels\n",
    "    clean_lyric = re.sub(r\"\"\"\n",
    "           [,.;@#?!&$\\n]+  # Accept one or more copies of punctuation\n",
    "           \\ *           # plus zero or more copies of a space,\n",
    "           \"\"\",\n",
    "           \" \",          # and replace it with a single space\n",
    "           song_lyric.lower(), flags=re.VERBOSE)\n",
    "    \n",
    "    #for each word in list\n",
    "    occurence_count = {}\n",
    "    for word_value_pair in query_array:\n",
    "        target_word = word_value_pair[0]\n",
    "        #count list elements in lyric\n",
    "        n_occurences = clean_lyric.split().count(target_word)\n",
    "        temporary_dict = {artist:song_title,target_word:n_occurences}\n",
    "        #check counts in the dictionary. if none found, it isn't saved\n",
    "        non_zero_counter = 0\n",
    "        for count in temporary_dict.values():\n",
    "            if type(count) != str: #the first is the title so that is a str\n",
    "\n",
    "                if count > 0:\n",
    "                    non_zero_counter += 1\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if non_zero_counter > 0:\n",
    "            occurence_count.update(temporary_dict)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #append dict if not empty\n",
    "    if len(occurence_count) > 0:     \n",
    "        occurence_count_full[\"song\"].append(occurence_count)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extended table\n",
    "ex_data = pd.DataFrame(occurence_count_full[\"song\"])\n",
    "ex_data\n",
    "#small info table\n",
    "small_df = pd.DataFrame(columns=[\"Artist\",\"Term\",\"Percentage\",\"All_Percentage\",\"N_Songs\"])\n",
    "#add terms\n",
    "small_df.Term = ex_data.columns[1:len(ex_data.columns)]\n",
    "#add artist\n",
    "small_df.Artist = ex_data.columns[0] \n",
    "#add n_songs\n",
    "small_df.N_Songs = max_row\n",
    "#add all_percentage\n",
    "small_df.All_Percentage = round(len(ex_data)/max_row*100,2)\n",
    "#add percentages for each query term\n",
    "#look through all colums of the terms in the big set, find non zeros, make it a list\n",
    "ex_data.fillna(0)\n",
    "percentages = []\n",
    "for col in ex_data.columns[1:len(ex_data.columns)]:\n",
    "    n_z_count = non_zero_counter(ex_data[col])\n",
    "    percentages.append(round(n_z_count/max_row*100,2))\n",
    "\n",
    "small_df.Percentage = percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Term</th>\n",
       "      <th>Percentage</th>\n",
       "      <th>All_Percentage</th>\n",
       "      <th>N_Songs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>love</td>\n",
       "      <td>35.40</td>\n",
       "      <td>39.82</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>hate</td>\n",
       "      <td>2.65</td>\n",
       "      <td>39.82</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>loving</td>\n",
       "      <td>2.65</td>\n",
       "      <td>39.82</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>loved</td>\n",
       "      <td>2.65</td>\n",
       "      <td>39.82</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>loves</td>\n",
       "      <td>1.77</td>\n",
       "      <td>39.82</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>adore</td>\n",
       "      <td>0.88</td>\n",
       "      <td>39.82</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Artist    Term  Percentage  All_Percentage  N_Songs\n",
       "0   ABBA    love       35.40           39.82      113\n",
       "1   ABBA    hate        2.65           39.82      113\n",
       "2   ABBA  loving        2.65           39.82      113\n",
       "3   ABBA   loved        2.65           39.82      113\n",
       "4   ABBA   loves        1.77           39.82      113\n",
       "5   ABBA   adore        0.88           39.82      113"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions !!!!\n",
    "def non_zero_counter(list_of_values):\n",
    "    non_zero_counter = 0\n",
    "    for count in list_of_values:\n",
    "        if type(count) != str: #the first is the title so that is a str\n",
    "\n",
    "            if count > 0:\n",
    "                    non_zero_counter += 1\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return non_zero_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing faculitiy\n",
    "\n",
    "class Extended_Query:\n",
    "\n",
    "    def __init__(self, data, model, artist, q_word, n_embeds):\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.n_embeds = n_embeds\n",
    "        self.artist = artist\n",
    "        self.q_word = q_word\n",
    "    \n",
    "    def create_query(self):\n",
    "        \"\"\" Method for creating a query from model and data,\n",
    "        based on artist and query extension size\n",
    "        - data: csv file with columns: 'artist', title as 'song', lyrics as 'text\n",
    "        - model: word embedding model\n",
    "        - artist: name of artist\n",
    "        - q_word: lowercase str without punctuations.\n",
    "        - n_embed: additional query extensions based on proximity in the embedding model\"\"\"\n",
    "        #subset\n",
    "        singer = self.artist\n",
    "        sub_data = self.data.loc[self.data['artist'] == singer]\n",
    "        #embed\n",
    "        query_word = self.q_word\n",
    "        n_similar = self.n_embeds\n",
    "        query_array = model.most_similar(query_word,topn=n_similar)\n",
    "        query_array.append((query_word, 1))\n",
    "\n",
    "        # find in subset (subset rows by indexing row number to sub_data.loc)\n",
    "        max_row = len(sub_data.index)\n",
    "        occurence_count_full = {\"song\":[]}\n",
    "        for row_index in range(0,max_row):\n",
    "            row = sub_data.loc[row_index]\n",
    "            song_title = row[\"song\"]\n",
    "            song_lyric = row[\"text\"]\n",
    "            artist = row[\"artist\"]\n",
    "            #clean lyrics to be compatible with the word embedding labels\n",
    "            clean_lyric = clean_text(song_lyric)\n",
    "            #for each word in list\n",
    "            occurence_count = {}\n",
    "            for word_value_pair in query_array:\n",
    "                target_word = word_value_pair[0]\n",
    "                #count list elements in lyric\n",
    "                n_occurences = clean_lyric.split().count(target_word)\n",
    "                temporary_dict = {artist:song_title,target_word:n_occurences}\n",
    "                #check counts in the dictionary. if none found, it isn't saved\n",
    "                n_z_counter = non_zero_counter(temporary_dict.values())\n",
    "\n",
    "                if n_z_counter > 0:\n",
    "                    occurence_count.update(temporary_dict)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            #append dict if not empty\n",
    "            if len(occurence_count) > 0:     \n",
    "                occurence_count_full[\"song\"].append(occurence_count)\n",
    "            else:\n",
    "                pass\n",
    "    return occurence_count_full\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean_text = re.sub(r\"\"\"\n",
    "                    [,.;@#?!&$\\n]+  # Accept one or more copies of punctuation\n",
    "                    \\ *           # plus zero or more copies of a space,\n",
    "                    \"\"\",\n",
    "                    \" \",          # and replace it with a single space\n",
    "                    text.lower(), flags=re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_tables(query_result):\n",
    "    #make tables\n",
    "    #extended table\n",
    "    ex_data = pd.DataFrame(query_result[\"song\"])\n",
    "    ex_data\n",
    "    #small info table\n",
    "    small_df = pd.DataFrame(columns=[\"Artist\",\"Term\",\"Percentage\",\"All_Percentage\",\"N_Songs\"])\n",
    "    #add terms\n",
    "    small_df.Term = ex_data.columns[1:len(ex_data.columns)]\n",
    "    #add artist\n",
    "    small_df.Artist = ex_data.columns[0] \n",
    "    #add n_songs\n",
    "    small_df.N_Songs = max_row\n",
    "    #add all_percentage\n",
    "    small_df.All_Percentage = round(len(ex_data)/max_row*100,2)\n",
    "    #add percentages for each query term\n",
    "    #look through all colums of the terms in the big set, find non zeros, make it a list\n",
    "    ex_data.fillna(0)\n",
    "    percentages = []\n",
    "    for col in ex_data.columns[1:len(ex_data.columns)]:\n",
    "        n_z_count = non_zero_counter(ex_data[col])\n",
    "        percentages.append(round(n_z_count/max_row*100,2))\n",
    "\n",
    "    small_df.Percentage = percentages\n",
    "\n",
    "    return ex_data, small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# van-e, melyik, hányszor\n",
    "# kell egy extended df meg egy quick df.\n",
    "#cs (Artist,title,all_freq/all_songs,ind_count,ind_freq/all_songs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
